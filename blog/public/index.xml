<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM Integrations Research Blog by COXIT</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on LLM Integrations Research Blog by COXIT</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Oct 2024 12:35:30 +0300</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Welcome To LLM Integrations Research Blog by COXIT</title>
      <link>http://localhost:1313/posts/my-first-post/</link>
      <pubDate>Thu, 10 Oct 2024 19:19:14 +0300</pubDate>
      <guid>http://localhost:1313/posts/my-first-post/</guid>
      <description>&lt;h3 id=&#34;welcome&#34;&gt;Welcome&lt;/h3&gt;&#xA;&lt;p&gt;Welcome to the internal COXIT blog about Prompt Engeneering and LLM integrations.&lt;/p&gt;&#xA;&lt;p&gt;This is COXIT&amp;rsquo;s internal R&amp;amp;D project. We aim to investigate different tools, approaches, and prompt engineering techniques for building LLM-powered production-ready solutions. We aim to try different tools currently known on the market and define what works and what causes any problems for our specific use cases.&lt;/p&gt;&#xA;&lt;p&gt;As a result of this project, we will build a knowledge base describing which of the available LLM evaluation tools, frameworks, LLMs themselves, and prompt engineering techniques worked best for our specific LLM-related cases, explain what didn&amp;rsquo;t work, and why.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Langsmith vs OpenAI Evals vs DeepEval: A Comprehensive Evaluation</title>
      <link>http://localhost:1313/posts/my-third-post/third-post/</link>
      <pubDate>Mon, 14 Oct 2024 12:35:30 +0300</pubDate>
      <guid>http://localhost:1313/posts/my-third-post/third-post/</guid>
      <description>&lt;h1 id=&#34;langsmith-vs-openai-evals-vs-deepeval-a-comprehensive-evaluation&#34;&gt;Langsmith vs OpenAI Evals vs DeepEval: A Comprehensive Evaluation&lt;/h1&gt;&#xA;&lt;h2 id=&#34;evaluating-and-testing&#34;&gt;Evaluating and Testing&lt;/h2&gt;&#xA;&lt;p&gt;Langsmith offers an online UI for prompt testing using either a custom dataset or manual inputs. This feature is particularly convenient because it allows users to store prompts along with their commit history and test results. Unfortunately, in DeepEval, testing cannot be done directly through the UI. Instead, users must use code to create &lt;code&gt;TestCase&lt;/code&gt; objects and evaluate the results through the UI. Also OpenAI provides a powerful SDK for evaluating prompts. However, it requires a bit more setup.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Prompt Engineering through Structured Instructions and Advanced Techniques</title>
      <link>http://localhost:1313/posts/my-second-post/my-second-post/</link>
      <pubDate>Thu, 10 Oct 2024 11:59:14 +0300</pubDate>
      <guid>http://localhost:1313/posts/my-second-post/my-second-post/</guid>
      <description>&lt;h1 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Language models (LLMs) are powerful tools for a variety of tasks, but their effectiveness is highly dependent on the design of prompts. This article examines advanced techniques in prompt engineering, focusing on the impact of instruction order, the &amp;ldquo;Ask Before Answer&amp;rdquo; technique, and the &amp;ldquo;Chain of Thoughts&amp;rdquo; (CoT) method, etc. By optimizing these factors, we can significantly enhance the accuracy and reliability of LLM outputs.&lt;/p&gt;&#xA;&lt;h1 id=&#34;2-the-importance-of-instruction-order&#34;&gt;2. The Importance of Instruction Order&lt;/h1&gt;&#xA;&lt;p&gt;Instruction order plays a crucial role in prompt engineering. Altering the sequence of instructions or actions can drastically change the outcome produced by the LLM. For instance, when we previously placed an instruction about not considering &amp;ldquo;semi-exposed surfaces&amp;rdquo; as the eleventh step, the LLM would still process these surfaces as it followed each step sequentially, reaching the exclusion instruction too late to apply it effectively. However, when this instruction was moved to precede all other steps, the LLM correctly disregarded &amp;ldquo;semi-exposed&amp;rdquo; surfaces. This demonstrates the necessity of positioning general concepts or definitions above the specific step-by-step instructions, ensuring they are applied throughout the process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Differences in Prompting Techniques: Claude vs. GPT</title>
      <link>http://localhost:1313/posts/difference-in-gpt-claude-prompting/difference-in-prompting-post/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/difference-in-gpt-claude-prompting/difference-in-prompting-post/</guid>
      <description>&lt;h1 id=&#34;differences-in-prompting-techniques-claude-vs-gpt&#34;&gt;Differences in Prompting Techniques: Claude vs. GPT&lt;/h1&gt;&#xA;&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;&#xA;&lt;p&gt;When working with language models (LLMs) like Claude and GPT, the effectiveness of prompts can vary significantly based on the model&amp;rsquo;s architecture and capabilities. This article explores key differences in prompting strategies for Claude and GPT, using advanced techniques such as meta-prompting, XML tagging, and the Chain of Thoughts (CoT) method. By understanding these differences, users can optimize their prompts to enhance the accuracy and reliability of LLM outputs.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
