[{"content":"Welcome Welcome to the internal COXIT blog about Prompt Engeneering and LLM integrations.\nThis is COXIT\u0026rsquo;s internal R\u0026amp;D project. We aim to investigate different tools, approaches, and prompt engineering techniques for building LLM-powered production-ready solutions. We aim to try different tools currently known on the market and define what works and what causes any problems for our specific use cases.\nAs a result of this project, we will build a knowledge base describing which of the available LLM evaluation tools, frameworks, LLMs themselves, and prompt engineering techniques worked best for our specific LLM-related cases, explain what didn\u0026rsquo;t work, and why.\nTo make a new post, install hugo, then run:\ncd blog hugo mod get -u # fetch the theme module hugo new posts/my-first-post.md #adjust filename to match title of your post hugo server -D Tech Stack This blog is powered by hugo + github actions + github pages.\nPages are simple markdown, so github preview functionality is sufficient.\nMy favorite features of this setup:\nstatic, secure autogenerated without any infra outside of github repo the client-side search (see index.json) powered by fuse.js. single binary to download to edit on local computer Option to avoid installing hugo locally and just create+edit pages via edit-on-github UX. Markdown Cons:\nUsing github auth prevents rss :( Commands used to setup blog This blog was setup using:\nhugo new site blog cd blog git submodule add https://github.com/theNewDynamic/gohugo-theme-ananke.git themes/ananke echo \u0026#34;theme = \u0026#39;ananke\u0026#39;\u0026#34; \u0026gt;\u0026gt; hugo.toml Using instructions from:\nhttps://github.com/adityatelange/hugo-PaperMod/wiki/Installation https://gohugo.io/getting-started/quick-start/ https://gohugo.io/hosting-and-deployment/hosting-on-github/ ","link":"http://localhost:1313/posts/my-first-post/","title":"Welcome To LLM Integrations Research Blog by COXIT"},{"content":"When working with large language models (LLMs) like GPT, a major challenge is ensuring consistency in responses. These models are inherently stochastic, meaning randomness is embedded in their behavior. However, in certain applications, such as testing or content generation where consistency is crucial, it becomes essential to minimize this variability. In this article, we\u0026rsquo;ll explore how to control randomness using parameters like seeds and top_p, and why monitoring the system_fingerprint parameter is critical. We\u0026rsquo;ll also discuss why achieving full determinism in LLM responses is ultimately impossible.\n1. Why Full Determinism Is Impossible While setting a seed and controlling parameters like top_p can help reduce randomness, true determinism in LLM responses is impossible to achieve for several reasons:\nNon-deterministic Model Behavior: Even when top_p is set to an extremely low value (e.g., top_p = 0.00000001), which allows only the highest-ranking token to be selected, the model can still produce different outputs over time. This is due to underlying non-deterministic faults within the model. On longer text generations, even with such strict top_p settings, the highest-ranked token may switch occasionally due to the complex vector math that happens before token sampling. This behavior comes from the probabilistic nature of neural networks, meaning that token choice can vary despite tight constraints.\nSystem Fingerprint Variability: The system_fingerprint parameter provides a unique identifier for the backend system serving the model. However, this fingerprint can change every time you call the model. When the fingerprint changes, it can indicate that the underlying API backend or model architecture has been updated. As a result, even with identical seeds and settings, slight changes in the model\u0026rsquo;s internal mechanics can lead to different outputs. Thus, even with strong controls over randomness, true determinism remains out of reach.\n2. Using a Seed to Control Randomness One of the most effective ways to reduce variability is by setting a seed. This ensures that, for a given input, the model produces the same output each time. While this helps create repeatability, it’s important to remember that due to the non-deterministic nature of LLMs, even this won’t guarantee perfect consistency across different API calls or versions of the model (especially if the system_fingerprint changes).\nLets test this parameters on our dataset. Call structure for GPT (gpt-4o-2024-05-13):\ncompletion = client.chat.completions.create( model=llm_model, temperature=0, seed=111, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: prompt_template.format( OPTIONS=options, INSTRUCTION=instruction, INPUT_TEXT=input_text ), } ], ) Output:\nFirst call\nSecond call\nThird call\nThe same call to Claude(claude-3.5-sonnet-2024-06-20) gave three identical response for 3 calls:\nAnother call structure:\ncompletion = client.chat.completions.create( model=llm_model, temperature=0, seed=999, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: prompt_template.format( OPTIONS=options, INSTRUCTION=instruction )}, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: input_text, } ] ) First call\nSecond and Third calls\nConclusion: As demonstrated by the testing results above, setting the temperature or top_p to zero and utilizing a fixed seed do not guarantee determinism in model responses. The key factor influencing the outcome lies in the vector mathematics employed during the token selection process (espesially in Third Layer of token generation : Beam Search with Randomness). This process of selecting tokens can be compared to Dijkstra’s algorithm. While traditional algorithms like Dijkstra’s evaluate all possible paths to ensure an optimal solution, LLMs do not exhaustively explore every potential token combination. This would indeed lead to significantly slower response times. Instead, LLMs prioritize computational efficiency by selecting highly probable tokens based on contextual patterns, which sometimes results in variability across responses. LLM doesn’t check all possible solutions to find the best one but generates outputs that are likely correct, because of that, we can see different result series to the same input cabinet specification. As a result, the model may produce slightly different answers with each invocation, driven by its focus on optimizing computational resources while balancing accuracy.\n3. Controlling Output Diversity with Top_p The top_p parameter controls the probability distribution from which the model selects the next token. Setting top_p to a lower value restricts the model to a smaller pool of possible token choices, narrowing down the output variability.\nHowever, lowering top_p too much can lead to unintended consequences:\nChain of Thought Resolution Issues: For complex tasks that require multi-step reasoning or explanations (e.g., chain of thought processes), setting top_p too low (e.g., top_p = 0.0001) can drastically degrade the model\u0026rsquo;s ability to think through problems. The lack of diversity in token selection limits the model’s ability to explore nuanced responses, leading to overly deterministic but shallow outputs.\nPerformance Deterioration: Lowering top_p to extreme levels can result in outputs that are either repetitive or overly simplistic, as the model is forced to choose from a very narrow set of tokens.\nBut the top_p parameter, even when set alongside a constant seed for deterministic outputs, does not guarantee consistent results across multiple runs. Below you can see proof of this.\nCall to Claude(claude-3.5-sonnet-2024-06-20) with this structure:\ncompletion = await client.chat.completions.create( model=llm_model, top_p=0.00001, seed=111, messages=[ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: prompt_template.format( OPTIONS=options, INSTRUCTION=instruction ), }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: input_text, } ] ) First call\nSecond call\nThird call\n4. Temperature and Top_p: Avoid Using Together Both temperature and top_p control the randomness of the model’s responses, but in different ways:\nTemperature: This parameter controls the randomness in token selection by scaling the probability distribution. Lower values (e.g., temperature = 0.1) make the model more deterministic, while higher values (e.g., temperature = 1.0) increase diversity.\nTop_p: This parameter limits the set of tokens considered for selection based on their cumulative probability.\nAccording to the documentation of both GPT and Claude, using temperature and top_p together can lead to unpredictable behavior and poor performance. When both are set simultaneously, the combined effect may confuse the model, leading to degraded reasoning abilities and lower-quality outputs. For example, using both parameters might significantly hamper tasks that require detailed reasoning, like multi-step logic or chain of thought processes.\n5. Additional Information on Temperature and n Parameter When you set the parameter n and the GPT-powered chat generated three responses for a single input, the behavior was as follows:\nWith the temperature parameter set to 0, all three responses were identical. This is because the GPT model generated all three responses in the same system state with a constant system fingerprint. However, when you restarted the LLM call, the response would change due to a different fingerprint. On the other hand, when you set the temperature parameter to 0.15 and the n parameter to 3, the three generated responses for the same input were different. This indicates that with a constant system fingerprint, the temperature or top_p parameter is the only factor affecting the diversity of the generated responses. Example with temp = 0, n = 3:\nExample with temp = 0.15, n=3:\nConclusion Minimizing randomness in GPT responses is crucial in scenarios where consistency is needed, but true determinism remains elusive due to the non-deterministic nature of the model and variability in the system_fingerprint. By carefully controlling temperature, seeds, and top_p settings, you can reduce randomness, though you must be cautious about over-constraining the model, as it may degrade performance in complex tasks. Monitoring system_fingerprint changes is also key to distinguishing random variation from API backend updates, helping you manage LLM behavior more effectively.\nIn our view, improving prompts and instructions is main essential step in reducing randomness. Another solution might be simplifying the input specification, by using additional LLM calls to extract only relevant data and removing unnecessary information, it would help to reduce hallucinations and prevent GPT from becoming confused. Unlike GPT, this issue is less critical in Claude. Since Claude manages large datasets effectively and consistently produces stable output, it makes sense to retain the current results. As for now, the main method to improve Claude’s accuracy is inserting into a prompt one or a few shotsvexamples selected through vector search based on the highest similarity. That can effectively improve accuracy without increasing complexity.\nReferences The “seed” option for GPT does not increase the determinism level\nChatCompletions are not deterministic even with seed set, temperature=0, top_p=0, n=1\nDeterminism I: Understanding Randomness Within LLMs\n","link":"http://localhost:1313/posts/minimazing_randomness/minimazing-randomness-post/","title":"Minimizing Randomness in GPT Responses: A Guide to Using Seeds, Top_p, and Monitoring System Fingerprints"},{"content":"1. Introduction When working with language models (LLMs) like Claude and GPT, the effectiveness of prompts can vary significantly based on the model\u0026rsquo;s architecture and capabilities. This article explores key differences in prompting strategies for Claude and GPT, using advanced techniques such as meta-prompting, XML tagging, and the Chain of Thoughts (CoT) method. By understanding these differences, users can optimize their prompts to enhance the accuracy and reliability of LLM outputs.\n2. Context Window Size and Information Processing One of the most significant differences between Claude and GPT lies in their context window size - the amount of text they can handle in a single prompt.\nClaude (claude-3.5-sonnet) can process up to 200,000 tokens, making it ideal for tasks requiring the analysis of large documents or aggregating data from multiple sources. This is especially valuable in business and academic research, where Claude can process entire reports or research papers in one go. Due to this large capacity, prompts for Claude need to be clear and explicit to focus the model on relevant parts of the input.\nGPT-4 (gpt-4o-2024-08-06) supports up to 128,000 tokens. Although this is smaller than Claude’s window, it is still a significant improvement over previous models. GPT is suited for tasks involving moderate-length documents or well-defined queries.\nFor our specific testcase, with Stevens AI Chatbot, Claude\u0026rsquo;s prompts in average have 9000 tokens per input and 600 tokens per output. For GPT, it’s 9000 tokens per input and 650 tokens per out, because of different token embeddings\nIn practice, Claude’s larger context window means prompt engineering requires attention to chunking and summarizing within the input to make the most of its potential, while GPT might be better suited for more focused and precise tasks\n3. Role of Examples and Instructions Both models benefit from examples and clear instructions:\nClaude: Examples play a central role. For complex tasks, offering several examples that showcase the expected response pattern can significantly improve the output. It’s important to ensure that these examples are concise and relevant, avoiding unnecessary complexity. Importantly, examples have to be diverse in order not to tend LLM to the basis. We provided our prompt with an example with an example of correctly resolving our Testcase-2 (1100-C).\nExample One-Shot Claude Prompt:\nYou are a Casework expert tasked with reviewing a specification and selecting the correct Series option that represents cabinet materials and thickness. Follow these instructions carefully: 1. Review the available Series options: \u0026lt;options\u0026gt; {OPTIONS} \u0026lt;/options\u0026gt; 2. Carefully read and understand the following instructions for selection: {INSTRUCTION} 3. Examine the provided specification: \u0026lt;specification\u0026gt; SPECIFICATION_USER_INPUT \u0026lt;/specification\u0026gt; 4. After your analysis, provide your reasoning and selection in the following format: \u0026lt;thinking\u0026gt; [Include your step-by-step reasoning here] \u0026lt;/thinking\u0026gt; \u0026lt;result\u0026gt; RESULT: [Insert selected Series option here] \u0026lt;/result\u0026gt; \u0026lt;example\u0026gt; Specification: PART 2 - PRODUCTS 2.2 MATERIALS General: Provide materials that comply with requirements of the AWI quality standard for each type of woodwork and quality grade specified, unless otherwise indicated. Wood Products: Comply with the following: Particleboard: ANSI A208.1, Grade M-2 M-2-Exterior Glue (at wet sink areas). High-Pressure Decorative Laminate: NEMA LD 3, grades as indicated, or if not indicated, as required by woodwork quality standard. Available Manufacturers: Subject to compliance with requirements, manufacturers offering high-pressure decorative laminates that may be incorporated into the Work include, but are not limited to, the following: - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 2.8 LAMINATE COUNTERTOPS Quality Standard: Comply with AWI Section 400 requirements for high-pressure decorative laminate countertops. Grade: Premium. High-Pressure Decorative Laminate Grade: HGS. Colors, Patterns, and Finishes: Provide materials and products that result in colors and textures of exposed laminate surfaces complying with the following requirements: Provide Architect\u0026#39;s selections from manufacturer\u0026#39;s full range of colors and finishes in the following categories: Solid colors. Grain Direction: Parallel to cabinet fronts. Edge Treatment: 3mm PVC. Color to match laminate. Core Material at Sinks: Particleboard made with exterior glue, or exterior-grade plywood. Answer: \u0026lt;thinking\u0026gt; Let\u0026#39;s analyze the specification step-by-step: 1. Core Material Selection: - The specification mentions \u0026#34;Particleboard: ANSI A208.1, Grade M-2 M-2-Exterior Glue (at wet sink areas).\u0026#34; - There\u0026#39;s no mention of NAUF or urea formaldehyde restrictions. - Default to Industrial Particleboard CARB Compliant. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 6. Cabinet Edgebanding Selection: - Not explicitly specified, default to Flat (.020\u0026#34;) PVC Cabinet Edgebanding. Based on these selections, we can narrow down our options to series that have: - Industrial Particleboard CARB Compliant core - HPL fronts and ends - Square Flat Edge The series that best matches these criteria is: \u0026lt;/thinking\u0026gt; \u0026lt;result\u0026gt; RESULT: 1100-C - Sq Flat Edge HPL Frts \u0026amp; HPL Ends IPB CARB Compliant Core \u0026lt;/result\u0026gt; \u0026lt;/example\u0026gt; Remember to base your decision solely on the information provided in the specification, instruction, and available options. Do not make assumptions or introduce external information. Claude output: Сonclusion: LLM chose IPB Core Compliant and Sq Flat Edge, although previously she easily solved this test case (1200-NAUF), which means that one test case can form the basis to which the LLM will tend when making its answer. Therefore, according to the Claude documentation, examples should be diverse.\nExample prompt with Few Shots. We provided our prompt with an example of correctly resolving our second test case (1100-C) with IPB Core Compliant and first (1200-N) with NAUF PB Core.\nOutput: Claude documentation\nWe asked Claude to evaluate our examples on relevance, diversity and clarity.\nOutput:\nCertainly, I\u0026#39;d be happy to evaluate the examples for relevance, diversity, and clarity. Let me analyze each example: Example 1: This example is highly relevant as it demonstrates the process of analyzing a specification and selecting the appropriate Series option. It shows a clear step-by-step thought process, which is excellent for illustrating how to approach the task. The example covers various aspects of cabinet construction, including core material, front and end materials, thickness, and edging. This diversity in the specification details helps to showcase how different elements are considered in the decision-making process. The thinking process is clearly laid out, making it easy to follow the reasoning behind the final selection. The format, with separate \u0026lt;thinking\u0026gt; and \u0026lt;result\u0026gt; tags, is clear and consistent with the instructions. Example 2: This example is also relevant and provides a different set of specifications to analyze. It adds diversity to the set of examples by presenting a case where No Added Urea Formaldehyde (NAUF) core is specified, which wasn\u0026#39;t a factor in the first example. The specification in this example is more detailed and covers additional elements like hardware and accessories, countertops, and solid surface materials. This breadth of information tests the ability to focus on the relevant details for cabinet construction while filtering out extraneous information. The thinking process is again clearly laid out, following the same format as the first example. This consistency aids in clarity and understanding. Overall evaluation: 1. Relevance: Both examples are highly relevant to the task of analyzing specifications and selecting the appropriate Series option for cabinet construction. 2. Diversity: The examples provide good diversity in terms of the specifications presented. They cover different materials, construction details, and environmental considerations (NAUF vs. non-NAUF). This diversity helps to illustrate how the selection process can vary based on different requirements. 3. Clarity: Both examples are clear in their presentation. The step-by-step thinking process is well-articulated, making it easy to follow the reasoning. The consistent use of the \u0026lt;thinking\u0026gt; and \u0026lt;result\u0026gt; tags adds to the clarity. Suggestion for improvement: While these examples are strong, you could consider adding a third example that presents a more challenging or ambiguous case. This could help illustrate how to handle situations where the specification is less clear-cut or where there might be conflicting information to resolve. This would add another layer of complexity and further demonstrate the critical thinking skills required for this task. Conclusion: Adding more solved test cases to the prompt boosts accuracy by providing more context, especially with few-shot methods like chain of thought, which improve reasoning. However, this approach becomes impractical due to high token costs.\nRetrieval-Augmented Generation (RAG) offers a better solution. It retrieves relevant information from external sources, reducing the number of tokens in the prompt while maintaining accuracy and relevance. By using only the most important context, RAG optimizes performance without overloading the model with excessive token usage.\n4. Meta-Prompting Meta-prompting is a technique used to refine and enhance the performance of a prompt. For Claude LLM, meta-prompting can significantly improve prompt effectiveness. The Claude documentation recommends using the \u0026ldquo;Generate a Prompt\u0026rdquo; feature that allows users to quickly create the first prompt or enhance their existing prompts. This functionality supports iterative improvements, helping to fine-tune prompts based on specific needs.\nIn contrast, meta-prompting for GPT has not shown substantial improvements in performance in our tests. Although this may be case-specific, it highlights a fundamental difference in how these models handle prompt enhancements\nClaude playground\n5. Formatting Prompts The way prompts are formatted and segmented can affect the performance of LLMs. Claude’s documentation recommends using XML tags to separate parts of the prompt template or large prompts. This method has proven effective in practice, improving the results significantly.\nFor GPT, it is advised to specify the delimiter being used, such as “You will be provided with a pair of articles (delimited with XML tags).” This indicates a fundamental difference in how Claude and GPT handle prompt formatting.\nClaude docs\nGPT docs\nClaude: Claude benefits from using XML tags to separate parts of the prompt, improving results:\n\u0026lt;options\u0026gt;{OPTIONS}\u0026lt;/options\u0026gt; \u0026lt;instruction\u0026gt;{INSTRUCTION}\u0026lt;/instruction\u0026gt; \u0026lt;specification\u0026gt;{INPUT_TEXT}\u0026lt;/specification\u0026gt; GPT: Prompt formatting should specify delimiters, but over-segmentation can reduce effectiveness:\nOPTIONS: {OPTIONS} INSTRUCTION: {INSTRUCTION} SPECIFICATION: {INPUT_TEXT} Example GPT (delimited with XML tags):\nYou will be provided with a instruction, options, specification (delimited with XML tags): \u0026lt;options\u0026gt; [OPTIONS] \u0026lt;/options\u0026gt; \u0026lt;instruction\u0026gt; [INSTRUCTION] \u0026lt;/instruction\u0026gt; \u0026lt;specification\u0026gt; [INPUT_TEXT] \u0026lt;/specification\u0026gt; 6. System and User Input Separation GPT and Claude’s performance improves when system prompts and user inputs are separated, enabling the model to process information more effectively. For example, the system prompt can provide specific instructions, while the user input is handled independently.\nExample: messages=[ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: prompt_template.format( OPTIONS=options, INSTRUCTION=instruction, ), }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: input_text, }, ] Additionally, it was observed that when XML tags were used to separate user input for Claude, it actually led to worse results. Specifically, tagging inputs separately (e.g., encapsulating user input within XML tags) made it harder for Claude to generate meaningful responses, likely due to how the model parses structured inputs compared to plain text. This finding highlights the importance of avoiding unnecessary formatting structures like XML tags when separating inputs for Claude.\nExample Prompt for Claude: You are a Casework expert tasked with reviewing a specification and selecting the correct Series option that represents cabinet materials and thickness. Follow these instructions carefully: 1. Review the available Series options: \u0026lt;options\u0026gt; {OPTIONS} \u0026lt;/options\u0026gt; 2. Carefully read and understand the following instructions for selection: {INSTRUCTION} 3. Examine the provided specification: \u0026lt;specification\u0026gt; SPECIFICATION_USER_INPUT \u0026lt;/specification\u0026gt; 4. After your analysis, provide your reasoning and selection in the following format: \u0026lt;thinking\u0026gt; [Include your step-by-step reasoning here] \u0026lt;/thinking\u0026gt; \u0026lt;result\u0026gt; RESULT: [Insert selected Series option here] \u0026lt;/result\u0026gt; Remember to base your decision solely on the information provided in the specification, instruction, and available options. Do not make assumptions or introduce external information. Worse Results: messages = [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: prompt_template.format( OPTIONS=options, INSTRUCTION=instruction, ), }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;\u0026lt;specification\u0026gt;{input_text}\u0026lt;specification/\u0026gt;\u0026#34;, }, ] Better Results: messages = [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: prompt_template.format( OPTIONS=options, INSTRUCTION=instruction, ), }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: input_text, }, ] 7. Chain of Thoughts (CoT) Technique The Chain of Thoughts (CoT) technique is essential for guiding LLMs through complex instructions by encouraging them to think step-by-step. For Claude, it is recommended to use XML tags to designate where the model should include its reasoning. This structured approach helps Claude organize its thoughts and produce coherent outputs.\nFor GPT, a simpler approach of instructing the model to \u0026ldquo;think step by step\u0026rdquo; or \u0026ldquo;take your time\u0026rdquo; is often sufficient. This technique encourages GPT to consider each step in the process but does not require the specific tagging used in Claude.\nExample for Claude: \u0026lt;thinking\u0026gt; [Include your step-by-step reasoning here] \u0026lt;/thinking\u0026gt; Output:\n\u0026lt;thinking\u0026gt; Let\u0026#39;s analyze the specification step-by-step: 1. Core Material Selection: - The specification mentions \u0026#34;Particleboard: ANSI A208.1, Grade M-2 M-2-Exterior Glue (at wet sink areas).\u0026#34; - There\u0026#39;s no mention of NAUF or urea formaldehyde restrictions. - Default to Industrial Particleboard CARB Compliant. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 4. Ends Material Selection: - The specification states \u0026#34;Bottoms and Ends of Cabinets, and Tops of Wall Cabinets and Tall Cabinets: 3/4-inch particleboard, plastic-laminate faced on exposed surfaces, thermoset decorative panels on semiexposed surfaces\u0026#34; - This indicates we should select High Pressure Laminate (HPL) for the ends - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Based on these selections, we can narrow down our options to series that have: - Industrial Particleboard CARB Compliant core - HPL fronts and ends - Square Flat Edge The series that best matches these criteria is: \u0026lt;/thinking\u0026gt; \u0026lt;result\u0026gt; RESULT: 1200-N - 3mm HPL Frts \u0026amp; HPL Ends NAUF PB Core \u0026lt;/result\u0026gt; Example for GPT: You think step by step. Take your time to answer. Output:\n### Step-by-Step Analysis: #### 1. Material Requirements: - **Core Material**: The specification mentions that adhesives should not contain urea formaldehyde. Therefore, we should select NAUF (No Added Urea Formaldehyde) Particleboard. - **Fronts Material**: The specification requires high-pressure decorative laminate (HPL) for exposed surfaces. - **Fronts Thickness**: The specification mentions GP-50 (0.050-inch nominal thickness) for horizontal surfaces and GP-28 (0.028-inch nominal thickness) for vertical surfaces. However, for the fronts, we will consider the standard thickness options provided in the series. - **Ends Material**: The ends should match the fronts, which is HPL. - **Fronts Edging**: The specification does not allow PVC edging, so we should look for options without PVC edging. - **Cabinet Edgebanding**: The specification does not allow PVC edging, so we should look for options without PVC edging. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ### Step 3: Fronts Material Selection - The specification states that door and drawer fronts must be matching high pressure laminate (HPL) on both sides. - Therefore, the fronts material should be High Pressure Laminate (HPL). - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - #### 7. Cabinet Edgebanding Selection: - **Selected Cabinet Edgebanding**: Since PVC edging is not allowed, we should look for options without PVC edging. ### Final Review: - **Core Material**: NAUF PB Core. - **Fronts Material**: 3mm High Pressure Laminate (HPL). - **Ends Material**: 3mm High Pressure Laminate (HPL). - **Fronts Edging**: 3mm Radius Edge. - **Cabinet Edgebanding**: Options without PVC edging. Based on the steps and the filtered options, the correct series option is: **RESULT: 1200-N - 3mm HPL Frts \u0026amp; HPL Ends NAUF PB Core** 8. Decomposing Complex Tasks in GPT: A Recommended Approach One of the key recommendations in the GPT documentation is to decompose complex tasks into simpler, more manageable steps. This method significantly enhances performance and reduces the likelihood of the model making mistakes by handling more digestible components of a task one by one. GPT excels when provided with specific instructions in a sequence, allowing it to focus on a single objective at a time.\nFor example, when working on determining the correct Series option that represents cabinet materials and thickness from a PDF document, a sequence of chain-of-calls (or sequential prompts) could simplify the task. This approach reduces cognitive overload for the model, ensuring more accurate and reliable outputs.\nHere’s how this decomposition would look: First prompt: Extract all relevant information about cabinet materials and thickness from the provided document.\nThis step isolates data extraction from interpretation, ensuring GPT focuses solely on identifying and retrieving the relevant details.\nSecond prompt: Based on the extracted information, determine the correct Series option that represents the cabinet materials and thickness using the provided instructions.\nHere, the model is now tasked with processing the extracted data, interpreting it according to specific criteria, and generating a decision.\nBy breaking down the task into these two steps, we reduce the complexity for the model, minimizing the chances of misinterpretation or errors. This method also makes it easier to troubleshoot and refine individual steps, should the need arise.\nExample on Claude: Example GPT: In general, this strategy of decomposing tasks into smaller, sequential prompts is especially useful in GPT’s chain of thought framework, where the model benefits from clear and distinct steps. When a PDF file is loaded into GPT for processing, the pricing is calculated based on the total word count of the entire PDF, which can become expensive, especially with large documents. But in my opinion, this method is one of the best ways to minimize hallucinations, and make the model\u0026rsquo;s responses more deterministic by extracting relevant sections before submitting them to another prompt to GPT. This strategy allows for a focused query, ensuring the model processes only the necessary information, leading to more reliable answers. However, in our specific test case, it might be more cost-efficient to develop an algorithm that automatically identifies and extracts only \u0026ldquo;Part 2 products,\u0026rdquo; instead of processing the entire document with GPT, making it a cheaper and more targeted solution.\n9. Roleplay Models vs. GPT: Following Instructions and Logical Compliance Roleplay models like Claude are more rigid in following instructions, making them ideal for tasks requiring strict adherence to step-by-step guidelines. In contrast, GPT models often rely on their own internal logic, which can lead to creative and effective problem-solving but may deviate from precise instructions. This flexibility is beneficial for open-ended tasks but requires additional review when strict accuracy is needed.\nFrom our experience with Claude and GPT on similar prompts, we observe that Claude tends to follow a more structured, step-by-step approach to instructions, while GPT’s outputs often involve more improvisation, making it necessary to monitor tasks for precise compliance.\nTESTCASE-3 (2800-C 3mm Frts \u0026amp; TF Color Throughout IPB CARB Compliant Core) GPT OUTPUT: Let\u0026#39;s follow the steps outlined in the instructions to determine the correct series option. ### Step 1: Material Requirements - The specification mentions that the basis for the specification is Stevens Advantage 2800 Designer Series. ### Step 2: Core Material Selection - The core material should be Industrial Particleboard CARB Compliant unless adhesives with urea formaldehyde are prohibited. - There is no mention of prohibiting adhesives with urea formaldehyde, so we will use Industrial Particleboard CARB Compliant. ### Step 3: Fronts Material Selection - The specification states that door and drawer fronts must be matching high pressure laminate (HPL) on both sides. - Therefore, the fronts material should be High Pressure Laminate (HPL). - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ### Step 7: Cabinet Edgebanding Selection - The specification states that cabinet sides, top, bottom, adjustable shelves, and other interior components shall be edged with (.020\u0026#34;) flat edge extrusion. - Therefore, the cabinet edgebanding style should be Flat (.020\u0026#34;) PVC Cabinet Edgebanding. ### Step 8: Final Review - Ensure all selections are compatible with each other. - Verify that all requirements from the original specification have been addressed. Based on the above steps, the correct series option is: **RESULT: 2800-C - 3mm Frts \u0026amp; TF Color Throughout IPB CARB Compliant Core** Here GPT’s outputs involved more improvisation, looking through documentations, it incorrectly determined the HPL for fronts material, but chose 2800-C, because found the sentence “the basis for the specification is Stevens Advantage 2800 Designer Series”. Claude adheres to the instructions more strictly and rarely uses such tricks.\n10. Conclusion In summary, the differences between Claude and GPT in terms of prompt engineering highlight the importance of understanding each model\u0026rsquo;s strengths and limitations. Claude excels in handling large context windows, making it suitable for processing extensive documents and generating detailed responses, while GPT shines in more focused tasks, particularly when instructions are clear and precise.\nWhen constructing prompts, Claude benefits from separated system and user inputs and XML-based formatting, while GPT works best when instructions are combined in a single, well-structured prompt without unnecessary formatting. The Chain of Thought (CoT) technique and task decomposition are highly recommended for both models but are executed slightly differently. GPT, for instance, performs better when complex tasks are broken down into simpler, sequential steps, which enhances clarity and reduces errors. Ultimately, prompt engineering is about leveraging the specific capabilities of each model to improve task efficiency and accuracy. By tailoring your approach to the strengths of Claude or GPT, you can maximize the potential of each model to achieve high-quality results.\n11. References Claude Prompting Guide\nGPT Prompting Guide\n","link":"http://localhost:1313/posts/difference_in_gpt_claude_prompting/difference-in-prompting-post/","title":"Differences in Prompting Techniques: Claude vs. GPT"},{"content":"Evaluating and Testing Langsmith offers an online UI for prompt testing using either a custom dataset or manual inputs. This feature is particularly convenient because it allows users to store prompts along with their commit history and test results. Unfortunately, in DeepEval, testing cannot be done directly through the UI. Instead, users must use code to create TestCase objects and evaluate the results through the UI. Also OpenAI provides a powerful SDK for evaluating prompts. However, it requires a bit more setup.\nPicture 1. Prompt saved in Langsmith\nPicture 2. Langsmith SDK for testing\nPicture 3. OpenAI SDK for testing and evaluating\nMetrics Metrics are crucial for evaluating LLM performance. All three frameworks—Langsmith, OpenAI Evals, and DeepEval—support both default metrics and custom metric creation. Here is how it\u0026rsquo;s implemented across the frameworks:\nPicture 4. Default metrics in Langsmith\nPicture 5. Creating custom metrics in Langsmith\nPicture 6. Default metrics in DeepEval\nPicture 7. Creating custom metrics in DeepEval\nPicture 8. Default metrics in OpenAI Eval\nPicture 9. Creating custom metrics in OpenAI Eval\nConclusion on metrics DeepEval stands out for its simplicity and user-friendly features, including easy-to-use default metrics and simple custom metric creation, requiring only two parameters: evaluation criteria and evaluation steps. On the other hand, OpenAI Evals and Langsmith offer more flexibility for custom evaluations by allowing users to create entirely custom prompts for LLM performance assessment. While DeepEval is ideal for ease of use, OpenAI Evals and Langsmith offer more precision and flexibility. Langsmith also allows testing directly within its playground, making it a fully-contained framework for prompt development. Additionally, OpenAI Evals provides almost free dataset testing, though result evaluation requires either manual methods or the paid LLM metrics feature.\nThe cost of evaluating responses with LLM metrics can vary. For example, using GPT-4o-mini to check the match between expected_response and actual_response typically costs about one cent for nine test cases, but this cost depends on the LLM model that we want to use and the context size being sent to the LLM in Judge.\nDataset Management Langsmith can automatically collect datasets from user inputs and allow them to be downloaded for further evaluation and testing using different prompts or LLM models in OpenAI Evals. DeepEval offers a similar feature, but this functionality is executed via code using the save_as(file_path) function.\nPicture 10. Downloading dataset in JSONL format\nPicture 11. Importing dataset into OpenAI Evals for testing\nMonitoring Calls OpenAI Evals does not support monitoring, so the comparison will focus on Langsmith and DeepEval. Langsmith offers a convenient way to monitor LLM calls by wrapping the OpenAI client in a function from the Langsmith library:\nclient = wrap_openai(get_client()) This allows for tracking input, output, tokens, pricing, etc. By adding the @traceable decorator, users can also monitor function execution time and input parameters.\nPicture 12. Monitoring call to OpenAI in Langsmith\nPicture 13. Monitoring function call in Langsmith\nDeepEval, on the other hand, provides a simpler monitoring process. By sending a call to DeepEval, specifying the input, output, and other parameters, monitoring is easily achieved:\ndeepeval.monitor( event_name=\u0026#34;Stevens Agent\u0026#34;, model=llm_model, input=input_text, response=llm_response, token_cost=total_cost, completion_time=completion_time, ) Picture 14. Monitoring calls in DeepEval\nPicture 15. Detailed call page in DeepEval\nAdding New Examples to Dataset In Langsmith, adding new test cases to a dataset involves selecting the required test cases, clicking “Add to Dataset,” and choosing the target dataset.\nPicture 16. Selecting the required test cases in Langsmith\nPicture 17. Selecting a dataset in Langsmith\nPicture 18. Added test cases in dataset in Langsmith\nTest cases can also be edited by modifying the input or output.\nPicture 19. Editing the test cases in Langsmith\nIn DeepEval, the process is similar, but it allows users to immediately add expected output, retrieval context, and other details. User comments and feedback are also reviewed on the same page.\nPicture 20. Selecting the required test cases in DeepEval\nPicture 21. Selecting a dataset in DeepEval\nPicture 22. Added test cases in dataset in DeepEval\nPicture 23. Editing the test cases in DeepEval\nCollecting Human Feedback One of the best ways to improve LLM performance is through human-in-the-loop techniques, where user feedback is collected to further optimize responses.\nIn Langsmith, feedback can be sent using the following code:\nllm_response = call_llm(input) run = get_current_run_tree() return llm_response, run.id With the run.id, feedback can be submitted:\ndef send_feedback_to_langsmith(): langsmith_client = LangsmithClient() langsmith_client.create_feedback( run_id, key=\u0026#34;Correctness\u0026#34;, score=0.8, comment=\u0026#34;Very well\u0026#34;, correction={ \u0026#34;Expected result\u0026#34;: \u0026#34;1200-N not 1200-C\u0026#34; } ) Picture 24. Monitoring Langsmith LLM calls filtered by feedback\nPicture 25. Detail feedback page in Langsmith\nIn DeepEval, the response ID is returned as a result of the deepeval.monitor function, and this ID is used for feedback tracking:\nresponse_id = deepeval.monitor( event_name=\u0026#34;Stevens Agent\u0026#34;, model=llm_model, input=input_text, response=llm_response, token_cost=total_cost, completion_time=completion_time, ) return llm_response, response_id Picture 26. Monitoring DeepEval LLM calls filtered by feedback\nPicture 27. Detail feedback page in DeepEval\nPricing LangSmith Pricing Picture 28. LangSmith Pricing\nDeepEval Pricing Picture 29. DeepEval Pricing\nOverall Conclusion Each of these frameworks provides valuable tools for developing and maintaining LLM applications. OpenAI Evals is well-suited for prompt optimization on large datasets, as it allows for relatively cost-free testing. DeepEval offers the simplest and most intuitive interface. Langsmith stands out as the most flexible framework, but mastering it requires a significant time investment. Ultimately, the choice between these tools depends on individual preferences and project requirements.\n","link":"http://localhost:1313/posts/comprehensive_frameworks_evaluation/comprehensive-frameworks-evaluation-post/","title":"Langsmith vs OpenAI Evals vs DeepEval: A Comprehensive Evaluation"},{"content":"In this series of tests, we conducted experiments with various large language models (LLMs) to assess their ability to follow complex instructions, reason through problems, and handle multi-step tasks. Below is a summary of the models tested, the outputs they produced, and a brief evaluation of their performance.\n1. MythoMax 13B (Nitro) Code: gryphe/mythomax-l2-13b:nitro Description: MythoMax is a popular fine-tune of Llama 2 13B, known for its descriptive output and roleplay capabilities. However, its performance in reasoning-based tasks is underwhelming. Results: GPT prompt output: Claude prompt output: Conclusion: MythoMax provided unsatisfactory results, failing to engage in any meaningful chain of thought (CoT) reasoning and producing very short, incomplete answers. 2. Reflection 70B (Free) Code: mattshumer/reflection-70b:free Description: This model, based on Llama 3.1 70B, uses a unique \u0026ldquo;Reflection-Tuning\u0026rdquo; technique to detect and correct mistakes in reasoning. It was trained on synthetic data. Results: GPT prompt output: Testcase 3 Detail GPT resolving: Output: \u0026lt;thinking\u0026gt; To solve this problem, I\u0026#39;ll follow these steps: Identify key information in the specification related to cabinet materials and thickness. Compare this information to the criteria outlined in the instruction. Match the identified information with the available Series options. Eliminate options that do not meet the criteria. Select the most appropriate Series option based on the analysis. Let\u0026#39;s go through this process: Key information from the specification: Cabinet ends: 3/4\u0026#34; thick Cabinet backs: 1/4\u0026#34; prefinished Medium Density Fiberboard (MDF) Adjustable shelves: 3/4\u0026#34; thick Drawer members: 3/4\u0026#34; thick Drawer bottoms: 1/4\u0026#34; MDF Comparing to the instruction criteria: The instruction asks for a Series option that represents cabinet materials and thickness. We need to focus on the thickness of the cabinet ends and the material of the cabinet backs. Matching with available Series options: Looking at the options, we can see that most of them have: Ends: High Pressure Laminate (HPL) or Thermally Fused Laminate (TF) Core: Industrial Particleboard (IPB) or No Added Urea Formaldehyde Particleboard (NAUF PB) Edgebanding: Flat (.020\u0026#34;) PVC Cabinet Edgebanding None of the options explicitly mention 3/4\u0026#34; thickness for ends or 1/4\u0026#34; MDF for backs. However, we need to find the closest match. Eliminating options: We can eliminate options that don\u0026#39;t match the general criteria: Series 1600-C and 1600-N can be eliminated as they have wood veneer fronts, which is not mentioned in the specification. Series 1100-C, 1100-N, 1150-C, and 1150-N can be eliminated as they have square flat edge HPL fronts, which doesn\u0026#39;t match the 3mm radius edge mentioned in the specification. Selecting the most appropriate option: After elimination, we\u0026#39;re left with Series 1200-C, 1200-N, 1250-C, and 1250-N. These all have 3mm HPL fronts and 3mm radius edges, which seems to be the closest match to the specification. Among these, Series 1200-C and 1200-N have HPL ends, while Series 1250-C and 1250-N have TF ends. The specification doesn\u0026#39;t explicitly mention the type of ends, so we can\u0026#39;t make a distinction based on this. The main difference between these options is the core material: Series 1200-C and 1250-C have Industrial Particleboard (IPB) CARB Compliant Core Series 1200-N and 1250-N have No Added Urea Formaldehyde Particleboard (NAUF PB) Core The specification doesn\u0026#39;t mention CARB compliance or NAUF, so we can\u0026#39;t make a decision based on this. \u0026lt;reflection\u0026gt; Upon reflection, I realize that I may have been too quick to eliminate the 1600 series options. While they do have wood veneer fronts, the specification doesn\u0026#39;t explicitly rule out wood veneer. Additionally, these options have 3mm wood edges, which could be considered similar to the 3mm radius edge mentioned in the specification. Let\u0026#39;s reconsider these options. \u0026lt;/reflection\u0026gt; Reconsidering the 1600 series options: Series 1600-C and 1600-N have wood veneer fronts with 3mm wood edges, which could be seen as similar to the 3mm radius edge mentioned in the specification. They also have HPL ends, which is consistent with the specification\u0026#39;s mention of 3/4\u0026#34; thick ends (though the exact thickness isn\u0026#39;t specified in the options). The core material is IPB CARB Compliant Core for 1600-C and NAUF PB Core for 1600-N, which is similar to the other options. Given this new consideration, Series 1600-C and 1600-N seem to be strong contenders. They match the 3mm edge requirement and have HPL ends, which is closer to the 3/4\u0026#34; thickness mentioned in the specification than the other options. \u0026lt;reflection\u0026gt; After this reflection, I believe Series 1600-C or 1600-N would be the best choice. They match the 3mm edge requirement and have HPL ends, which is closer to the 3/4\u0026#34; thickness mentioned in the specification than the other options. The wood veneer fronts could be seen as a minor deviation from the exact specification, but it\u0026#39;s the closest match overall. \u0026lt;/reflection\u0026gt; \u0026lt;/thinking\u0026gt; \u0026lt;output\u0026gt; Based on the analysis of the specification and the available Series options, the most appropriate Series option that represents cabinet materials and thickness is either Series 1600-C or Series 1600-N. These options stand out for several reasons: Edge style: Both Series 1600-C and 1600-N have a 3mm radius wood edge, which closely matches the 3mm radius edge mentioned in the specification. End material: They have HPL (High Pressure Laminate) ends, which is consistent with the specification\u0026#39;s mention of 3/4\u0026#34; thick ends. While the exact thickness isn\u0026#39;t specified in the options, HPL ends are a closer match to the 3/4\u0026#34; thickness requirement than the other options. Core material: Both options have a particleboard core, with Series 1600-C having an Industrial Particleboard (IPB) CARB Compliant Core and Series 1600-N having a No Added Urea Formaldehyde Particleboard (NAUF PB) Core. This is similar to the core materials mentioned in the other options. Overall construction: The wood veneer fronts with 3mm wood edges in these options could be seen as a minor deviation from the exact specification, but they provide a strong and durable construction that aligns with the overall quality implied in the specification. While the other options (Series 1200-C, 1200-N, 1250-C, and 1250-N) have 3mm HPL fronts and 3mm radius edges, they don\u0026#39;t match the end thickness requirement as closely as the 1600 series options. The 1100 and 1150 series options can be eliminated due to their square flat edge HPL fronts, which don\u0026#39;t match the 3mm radius edge mentioned in the specification. Therefore, between Series 1600-C and 1600-N, the choice would depend on whether CARB compliance (1600-C) or NAUF (1600-N) is preferred or required for the project. Both options represent the cabinet materials and thickness most closely based on the provided specification and available Series options. \u0026lt;/output\u0026gt; Expected Response: 2800-C - 3mm Frts \u0026amp; TF Color Throughout IPB CARB Compliant Core\nResult: Reflection LLMs aim to explain each step of the instructions comprehensively but face significant challenges in executing nearly every step. This difficulty might stem from the specific synthetic data on which the model was trained. Despite its structured approach, the model often fails to accurately address key details outlined in the instructions. Notably, the model exhibits high determinism, consistently providing the same result across multiple prompts with zero temperature. Claude prompt output: Testcase 1 Detail Claude resolving: Output: \u0026lt;thinking\u0026gt; To solve this problem, I\u0026#39;ll follow these steps: Analyze the specification to identify key information about cabinet materials and thickness. Compare this information with the criteria in the instruction. Match the identified information with the available Series options. Eliminate options that don\u0026#39;t meet the criteria. Select the most appropriate Series option. Let\u0026#39;s go through this process: Analyzing the specification: Cabinet construction: Particleboard, plywood, hardwood lumber, or high-density fiber board. Thickness: Minimum 1/2-inch for sides, back, and sub-front of drawers. Bottoms: Minimum 1/4-inch tempered hardboard. Drawer bottoms: Minimum 1/2-inch thick particleboard. Comparing with the instruction criteria: The instruction asks for a Series option that represents cabinet materials and thickness. We need to find an option that matches the materials and thicknesses specified. Matching with available Series options: The options mention various materials like HPL (High Pressure Laminate), TF (Thermally Fused Laminate), and wood veneer. Thicknesses mentioned include 3mm (which is approximately 1/8 inch) and no specific thickness for wood veneer. Eliminating options: Options with 3mm thickness (1100-C, 1100-N, 1150-C, 1150-N, 1200-C, 1200-N, 1250-C, 1250-N) can be eliminated as they don\u0026#39;t meet the minimum 1/2-inch thickness requirement. Options with HPL or TF for all parts (1100-C, 1100-N, 1150-C, 1150-N, 1200-C, 1200-N, 1250-C, 1250-N) can be eliminated as they don\u0026#39;t match the variety of materials specified in the instruction. Selecting the most appropriate option: The only option that remains is 1600-C or 1600-N, as they are the only ones that mention wood veneer and have a 3mm wood edge, which could potentially meet the thickness requirement. \u0026lt;reflection\u0026gt; Upon reflection, I realize that I may have been too hasty in eliminating the 3mm options. The specification mentions a minimum 1/2-inch thickness for some parts, but not all. The 3mm thickness could potentially apply to certain components. However, the main issue is that none of the 3mm options mention the variety of materials specified in the instruction. The 1600-C and 1600-N options are still the most likely candidates as they mention wood veneer and have a 3mm wood edge, which could meet the thickness requirement for some parts. \u0026lt;/reflection\u0026gt; After careful consideration, the most appropriate Series option appears to be either 1600-C or 1600-N. Both of these options mention wood veneer with a 3mm wood edge, which could potentially meet the thickness requirement for some parts of the cabinet. They also mention HPL for the ends, which aligns with the variety of materials specified in the instruction. \u0026lt;/thinking\u0026gt; \u0026lt;output\u0026gt; Based on the analysis of the specification and the available Series options, the most appropriate Series option that represents cabinet materials and thickness is either 1600-C or 1600-N. These options are the best match for several reasons: Material variety: They mention wood veneer for the fronts, which aligns with the specification\u0026#39;s mention of various materials including wood veneer. Thickness: While the specification mentions a minimum 1/2-inch thickness for some parts, the 3mm wood edge mentioned in these options could potentially meet this requirement for certain components. Core material: Both options mention Industrial Particleboard CARB Compliant Core (1600-C) or No Added Urea Formaldehyde Particleboard (1600-N), which aligns with the specification\u0026#39;s mention of particleboard as a possible material. Edge style: The 3mm Radius Wood Edge mentioned in these options could potentially meet the thickness requirement for some parts of the cabinet. Overall construction: These options seem to offer a more comprehensive construction method that could accommodate the various materials and thicknesses specified in the instruction. It\u0026#39;s important to note that while these options are the best match, they don\u0026#39;t perfectly align with all aspects of the specification. The specification mentions a minimum 1/2-inch thickness for some parts, which is not explicitly stated in these options. However, given the available choices, 1600-C or 1600-N appear to be the most suitable options for representing cabinet materials and thickness as specified in the instruction.\u0026lt;/output\u0026gt; Expected Response: 1200-N - 3mm HPL Frts \u0026amp; HPL Ends NAUF PB Core\nResult: Reflection LLM follows the prompt structure closely, striving to execute every step of the provided instructions and guidelines. However, it often struggles with selecting the right keywords and sentences from specification and has difficulty identifying the key elements to focus on when analyzing the cabinet specification. 3. Meta: Llama 3.1 8B Instruct (Free) Code: meta-llama/llama-3.1-8b-instruct:free Description: Meta\u0026rsquo;s Llama 3.1 series includes instruct-tuned versions aimed at high-efficiency tasks. The 8B variant is fast but falls short on more complex problem-solving tasks. Results: GPT prompt output: Claude prompt output: Conclusion: The LLaMA 3.1 8B model tends to provide very brief and short responses and struggles with effectively utilizing the \u0026ldquo;chain of thought\u0026rdquo; technique, often making non-transparent decisions that can be difficult to follow. Also LlaMA 3.1 8B has very low determinism and very often writes different solutions for the same test case.\n4. Meta: Llama 3.1 405B Instruct Code: meta-llama/llama-3.1-405b-instruct Description: A highly anticipated 405B Llama model designed for dialogue use cases and long context processing (128k tokens). It performs well on structured dialogue tasks. Results: GPT prompt output: Claude prompt output: Conclusion: Although LLaMA 405B showcased good problem-solving structure, it was outperformed by models like GPT and Claude in terms of accuracy, especially with complex multi-step tasks. 5. Anthropic: Claude 3.5 Sonnet (Self-Moderated) Code: anthropic/claude-3.5-sonnet:beta Description: A self-moderated version of Claude, designed to execute complex multi-step instructions with precision and consistency. Results: Claude prompt output: TestCase 8 resolving: Output: TESTCASE - 8 OUTPUT of Claude-3-opus: \u0026lt;thinking\u0026gt; 1. Core Material Selection: The specification mentions particleboard core materials with a minimum density of 45-48 lb, tested under ANSI A 208.1 and/or ASTM D-1037-91A standards. It does not explicitly prohibit adhesives containing urea formaldehyde. Therefore, I will select Industrial Particleboard CARB Compliant as the core material. 2. Fronts Material Selection: The specification states that door and drawer fronts must be matching high pressure laminate on both sides. Therefore, I will select 3mm High Pressure Laminate as the fronts material. 3. Fronts Thickness Determination: The specification mentions that door and drawer fronts shall be 3/4\u0026#34; thick. This aligns with the standard thickness for 3mm High Pressure Laminate fronts. 4. Ends Material Selection: The specification does not explicitly mention the material for cabinet ends. However, it states that all exposed/semi-exposed surfaces shall be color matching thermofused or high pressure laminate. Since the fronts are specified as HPL, I will select High Pressure Laminate as the ends material to maintain consistency. 5. Fronts Edging Selection: The specification clearly states that door and drawer front edges shall have 3mm radius extrusion banding. Therefore, I will select 3mm Radius Edge as the fronts edging style. 6. Cabinet Edgebanding Selection: The specification mentions that cabinet sides, top, bottom, adjustable shelves, and other interior components shall be edged with (.020\u0026#34;) flat edge extrusion. This matches the default option of Flat (.020\u0026#34;) PVC Cabinet Edgebanding. Based on the provided specification and the selection criteria, the most appropriate Series option is: 5120-C - 3mm HPL Frts \u0026amp; Ends IPB CARB Compliant Core \u0026lt;/thinking\u0026gt; \u0026lt;result\u0026gt; RESULT: 5120-C - 3mm HPL Frts \u0026amp; Ends IPB CARB Compliant Core \u0026lt;/result\u0026gt; Expected Response: 1250-C - 3mm HPL Frts \u0026amp; TF Ends IPB CARB Compliant Core\nConclusion: Claude 3.5 Sonnet delivered highly accurate results with a structured, step-by-step process, excelling at complex tasks. It also exhibited a high level of determinism, consistently producing the same output across multiple prompts. Note Pricing for claude-3.5-sonnet: Cost for 9 test cases: 0.32 $ | Average cost for 1 testcase: 0.036 $\nFor example pricing for gpt-4o-2024-05-13: Cost for 9 test cases: 0.41 $ | Average cost for 1 testcase: 0.046 $\n6. Qwen 2 72B Instruct Code: qwen/qwen-2-72b-instruct Description: This model from the Qwen family handled GPT-based prompts with impressive accuracy, completing most tasks successfully. Results: GPT prompt output: Claude prompt output: TestCase 3 resolving: Output: Justification: 1. Core Material Selection: The specification mentions particleboard core materials with a minimum density of 45-48 lb. Since there\u0026#39;s no mention of adhesives containing urea formaldehyde, we default to \u0026#34;\u0026#34;Industrial Particleboard CARB Compliant.\u0026#34;\u0026#34; 2. Fronts Material Selection: The specification requires door and drawer fronts to be finished entirely in matching high pressure laminate (HPL) materials on both the fronts and backs. The rest of the exposed/semi-exposed cabinet components can be either color matching thermofused (TFL) or matching HPL. Since the fronts must be HPL, we select \u0026#34;\u0026#34;3mm High Pressure Laminate.\u0026#34;\u0026#34; 3. Fronts Thickness Determination: The specification indicates that door and drawer fronts shall be 3/4\u0026#34;\u0026#34; thick. This corresponds to the \u0026#34;\u0026#34;3mm\u0026#34;\u0026#34; thickness option for fronts. 4. Ends Material Selection: The specification allows for either color matching TFL or matching HPL for the rest of the exposed/semi-exposed cabinet components. Since we have already selected HPL for the fronts, we default to HPL for the ends. 5. Fronts Edging Selection: The specification requires 3mm radius extrusion banding for door and drawer fronts. This corresponds to the \u0026#34;\u0026#34;3mm Radius Edge\u0026#34;\u0026#34; style. 6. Cabinet Edgebanding Selection: The specification requires (.020\u0026#34;\u0026#34;) flat edge extrusion for cabinet sides, top, bottom, adjustable shelves, and other interior components. This corresponds to the \u0026#34;\u0026#34;Flat (.020\\\u0026#34;\u0026#34;) PVC Cabinet Edgebanding\u0026#34;\u0026#34; option. The \u0026#34;\u0026#34;2800-C - 3mm Frts \u0026amp; TF Color Throughout IPB CARB Compliant Core\u0026#34;\u0026#34; series matches all the requirements specified in the input text. It features a 3mm High Pressure Laminate for fronts, matching High Pressure Laminate for ends, an Industrial Particleboard CARB Compliant core, and a 3mm Radius Edge style for the fronts, with Flat (.020\\\u0026#34;\u0026#34;) PVC Cabinet Edgebanding for the cabinet edges. Pricing for qwen/qwen-2-72b-instruct: Cost for 9 test cases: 0.04 $ | Average cost for 1 testcase: 0.0045 $\nConclusion: Qwen 2 72B Instruct demonstrated solid performance, successfully completing 8 out of 9 tasks. It also maintained a relatively low cost per test case, making it a highly efficient choice. With a cost of $0.35/M input tokens and $0.4/M output tokens (0.04 $ for 9 test cases), Qwen-2-72B presents itself as a viable option for further testing, when:\ngpt-4o-2024-08-06 $2.5/M input and $10/M output (0.21 $ for 9 test cases) gpt-4o-2024-05-13 $5/M input and $10/M output (0.40 $ for 9 test cases) claude-3.5-sonnet $3/M input and $15/M output (0.32 $ for 9 test cases) ","link":"http://localhost:1313/posts/experiments_with_different_llms/experiments-with-different-llms-post/","title":"Experiments with Different LLMs"},{"content":"1. Introduction Language models (LLMs) are powerful tools for a variety of tasks, but their effectiveness is highly dependent on the design of prompts. This article examines advanced techniques in prompt engineering, focusing on the impact of instruction order, the \u0026ldquo;Ask Before Answer\u0026rdquo; technique, and the \u0026ldquo;Chain of Thoughts\u0026rdquo; (CoT) method, etc. By optimizing these factors, we can significantly enhance the accuracy and reliability of LLM outputs.\n2. The Importance of Instruction Order Instruction order plays a crucial role in prompt engineering. Altering the sequence of instructions or actions can drastically change the outcome produced by the LLM. For instance, when we previously placed an instruction about not considering \u0026ldquo;semi-exposed surfaces\u0026rdquo; as the eleventh step, the LLM would still process these surfaces as it followed each step sequentially, reaching the exclusion instruction too late to apply it effectively. However, when this instruction was moved to precede all other steps, the LLM correctly disregarded \u0026ldquo;semi-exposed\u0026rdquo; surfaces. This demonstrates the necessity of positioning general concepts or definitions above the specific step-by-step instructions, ensuring they are applied throughout the process.\nExample:\n3. The \u0026ldquo;Ask Before Answer\u0026rdquo; Technique The \u0026ldquo;Ask Before Answer\u0026rdquo; technique is particularly effective when optimizing prompts to reduce the occurrence of hallucinations. By prompting the LLM to seek clarification before resolving a task, we can preempt misunderstandings that might lead to incorrect answers.\nExample Prompt:\nYou are a Casework expert. You review the specification and follow the instruction to pick the correct Series option which represents cabinet materials and thickness: OPTIONS: {OPTIONS} INSTRUCTION: {INSTRUCTION} SPECIFICATION: {INPUT_TEXT} If you have questions or misunderstandings, ask about it before resolving the task. Before proceeding with the itinerary, please ask for any clarifications or additional details. I will give more info if you need. Result:\nWhen applying this technique, we ask the LLM to identify specific areas where it may be uncertain or confused in resolving a test case. By doing so, we can pinpoint where hallucinations occur, understand why the LLM struggles with certain choices, and refine the prompt in those areas where the model tends to get confused. This method is highly effective in improving the quality of the instructions provided in the prompt.\n4. The Chain of Thoughts (CoT) Technique Result without CoT:\nOne of the most critical steps in creating an effective prompt with complex instructions is the use of the Chain of Thoughts (CoT) technique. By including phrases like \u0026ldquo;You think step by step,\u0026rdquo; \u0026ldquo;Take your time,\u0026rdquo; or \u0026ldquo;Explain every step,\u0026rdquo; the LLM is given time to reflect and process all input data. This approach significantly improves the results, making them more logical and coherent. However, caution is needed when using \u0026ldquo;Explain every step,\u0026rdquo; as the LLM can sometimes provide the most likely correct answers without fully understanding why, leading to hallucinations.\nResult with CoT:\n5. Meta-Prompting: An Advanced Strategy in Prompt Engineering Meta-prompting is an advanced technique in prompt engineering that goes beyond merely guiding a language model (LLM) through a task. It involves crafting prompts that instruct the model on how to think or approach a problem before the primary task begins. This strategic layer of prompting enhances the LLM\u0026rsquo;s ability to navigate complex instructions by embedding a meta-level understanding of the task at hand. For example, instead of directly asking the LLM to solve a problem, a meta-prompt might instruct the model to first assess whether it fully understands the task, identify potential ambiguities, and request clarification if necessary.\nWhen applied in Claude LLM, meta-prompting proved to be more effective than in GPT models. It significantly improved test case outcomes by making instructions simpler and clearer for the model to understand. Claude\u0026rsquo;s advanced processing capabilities allowed it to better interpret and act on the meta-prompts, leading to more accurate and consistent results.\nExample how meta-prompting optimized Claude outputs:\nHowever, in our specific case, meta-prompting did not lead to the exceptional results we had hoped for. While it is a valuable technique, its effectiveness can vary depending on the complexity of the task and the model\u0026rsquo;s inherent capabilities.\n6. Explanatory Instructions One key insight in optimizing prompt engineering is the importance of providing explanations for why certain choices should be made. Adding reasoning behind instructions helps LLMs make better-informed decisions, thereby improving their overall performance.\nFor example:\nWorse: \u0026ldquo;If both HPL and TFL can be used, choose TFL.\u0026rdquo; Better: \u0026ldquo;If both HPL and TFL can be used, choose TFL as the more cost-effective option.\u0026rdquo; In the \u0026ldquo;better\u0026rdquo; example, the LLM is not only told which option to choose but also why that choice is preferable. This additional context helps the model understand the underlying logic and apply it more consistently across different scenarios.\n7. Simplify Your Instructions When writing instructions for LLMs, it’s crucial to keep them clear and simple. If people find it hard to read and understand the instructions, the model will struggle even more. Use plain language and short sentences, as if you’re explaining things to a child.\nFor instance:\nWorse: \u0026ldquo;If front materials are not specified or not matched with TFL surfaces, HPL, or A-Tech Surface, choose HPL as the default option.\u0026rdquo; Better: \u0026ldquo;If fronts are not specified, default to HPL unless TFL or A-Tech Surface is mentioned.\u0026rdquo; In the \u0026ldquo;better\u0026rdquo; example, the instructions are straightforward and easier to understand.\n8. Imagine You\u0026rsquo;re Training a Colleague When crafting prompts, it\u0026rsquo;s essential to approach the task as if you are training a child or a new colleague at work to complete a specific task. To achieve this, you need to provide the individual with sufficient context and detailed, clear instructions.\n9. The Role in Prompting An important aspect of creating effective prompts is assigning a specific role to the language model (LLM). This role definition helps limit the data the LLM will use to solve the task and sets the appropriate tone and format for the response.\nExample:\nWorse: \u0026ldquo;You are a Casework expert.\u0026rdquo; Better: \u0026ldquo;You are a Casework expert tasked with reviewing a specification and selecting the correct Series option that represents cabinet materials and thickness.\u0026rdquo; This role-based approach provides the LLM with a clear context and specific guidelines, which positively influences the accuracy and relevance of the model\u0026rsquo;s responses.\n10. Conclusion In prompt engineering, the careful structuring of instructions, along with techniques like \u0026ldquo;Meta-prompting\u0026rdquo; and \u0026ldquo;Chain of Thoughts,\u0026rdquo; can dramatically enhance the performance of language models. These methods help in reducing hallucinations, improving clarity, and ensuring more accurate outcomes.\n11. References Prompt Engineering for ChatGPT: A Comprehensive Guide The Prompt Report: A Systematic Survey of Prompting Techniques ","link":"http://localhost:1313/posts/prompt_engineering/prompt-engineering-post/","title":"Prompt Engineering through Structured Instructions and Advanced Techniques"}]